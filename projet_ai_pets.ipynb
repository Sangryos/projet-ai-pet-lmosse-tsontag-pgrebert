{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSvanAuGWVJW",
        "outputId": "e99a8a1a-e6a5-42b6-95f3-8ac653427bce"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Vous devez constituer des équipes de 4 étudiants.\n",
        "Vous serez évalués sur un rendu sur Github.\n",
        "L'objet du projet est soit d'entraîner un classifieur VIT sur un dataset d'images labélisées (différent de ceux vus en TP et de ceux choisis par les autres équipes),\n",
        "soit d'entrainer/fine-tuner un modèle de diffusion (par exemple lucidrain) sur un dataset qui vous intéresse (mangas, joueurs de foot, ...).\n",
        "Dans le readme vous décrirez comment vous avez constitué votre dataset,\n",
        "et donnerez des exemples d'images du dataset/images générées par votre modèle (dans le cas génératif)\n",
        "ainsi que la fonction permettant de sampler de nouvelles images, ou bien l'accuracy dans le cas de classifieur\n",
        "(avec une fonction indiquant comment récupérer le dataset de validation et lancer le calcul de la métrique (afin que nous puissions vérifier)).\n",
        "'''\n",
        "!pip install Augmentor\n",
        "\n",
        "import torch # https://pytorch.org/docs/stable/index.html\n",
        "import torchvision # https://pytorch.org/vision/stable/index.html\n",
        "import os\n",
        "import shutil\n",
        "import Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZzsf-ZRyBUJ",
        "outputId": "98d04644-3d80-4615-be49-beea3a76dccb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lt8gF1UyBUN",
        "outputId": "50a055e9-12a3-455a-83d3-fc4d87e84903"
      },
      "outputs": [],
      "source": [
        "img_size = 250\n",
        "channel_size = 3\n",
        "transform = []\n",
        "transform_list = [\n",
        "        torchvision.transforms.Resize((img_size,img_size)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=0.5, std=0.5),\n",
        "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomVerticalFlip(p=0.5)\n",
        "        ]\n",
        "if channel_size == 1 :\n",
        "    transform_list.append(torchvision.transforms.Grayscale())\n",
        "\n",
        "transform = torchvision.transforms.Compose(transform_list)\n",
        "print(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "fSygmcz5yBUP",
        "outputId": "8ffbfb5e-eca2-4907-e9df-4d11d928151f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Le code suivant est censé devoir ajouter des nouvelles images pour pouvoir avoir des meilleures résultats dans l'entrainement du model,\n",
        "mais ça ne fonctionne pas, les images ne sont pas sauvegardés dans le dossier spécifié.\n",
        "\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "import Augmentor\n",
        "\n",
        "# Définir le chemin d'entrée pour les données originales\n",
        "input_path = \"/content/data/oxford-iiit-pet/images\"\n",
        "\n",
        "# Définir le chemin de sortie pour les images générées\n",
        "output_path = \"/content/data/oxford-iiit-pet/images/output\"\n",
        "\n",
        "# Créer un pipeline Augmentor avec le chemin de sortie spécifié\n",
        "p = Augmentor.Pipeline(input_path, output_directory=output_path)\n",
        "\n",
        "# Ajouter des opérations d'augmentation\n",
        "p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "p.flip_left_right(probability=0.5)\n",
        "p.flip_top_bottom(probability=0.5)\n",
        "\n",
        "# Spécifier le format d'enregistrement des images (PNG)\n",
        "p.set_save_format(\"PNG\")\n",
        "\n",
        "# Spécifier le nombre d'images que vous souhaitez générer\n",
        "num_images_to_generate = 1\n",
        "\n",
        "# Générer de nouvelles images\n",
        "p.sample(num_images_to_generate)\n",
        "\n",
        "# Processus d'écriture des images dans le dossier de sortie spécifié\n",
        "p.process()\n",
        "\n",
        "# Créer un transform similaire à celui utilisé pour les données d'entraînement\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Charger les nouveaux jeux de données d'entraînement et de test à partir des images augmentées\n",
        "train_dataset = datasets.ImageFolder(root=output_path, transform=transform)\n",
        "test_dataset = datasets.OxfordIIITPet(root='./data/', split=\"test\", transform=transform, download=True)\n",
        "\n",
        "# Afficher le nombre total d'images dans le nouvel ensemble de données d'entraînement\n",
        "print('Nombre total d\\'images dans l\\'ensemble de données d\\'entraînement:', len(train_dataset))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c8fdxYgtBz2",
        "outputId": "1b4a122c-b0c9-4249-ce0d-e8ad79f3aca1"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.OxfordIIITPet(root='./data/',split=\"trainval\", transform=transform,download=True)\n",
        "test_dataset = torchvision.datasets.OxfordIIITPet(root='./data/',split=\"test\", transform=transform,download=True)\n",
        "print('Nombre d\\'images d\\'entraînement:', len(train_dataset))\n",
        "print('Nombre d\\'image de test:', len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vCn6CRJuyBUQ"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader =torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2GnOgvpyBUS",
        "outputId": "ac0cdff6-cebe-4247-94f6-5cdb736faa9a"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(test_dataloader)\n",
        "batch_idx, data = next(examples)\n",
        "print(len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vnlzuiLyBUS",
        "outputId": "d7c37f0f-2317-41a2-fd00-73109c82bd4c"
      },
      "outputs": [],
      "source": [
        "print('Images:', data[0].shape)\n",
        "b, c, h, w = data[0].shape\n",
        "print('Batch(s):', b)\n",
        "print('Channel(s):', c)\n",
        "print('Height:', h)\n",
        "print('Width:', w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0r1_uHWyBUT",
        "outputId": "2da6b76f-863e-42e1-d6a3-562de0cccdef"
      },
      "outputs": [],
      "source": [
        "print('Labels:', data[1].shape) # 32 labels\n",
        "print(data[1])\n",
        "classes = train_dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "yWaatXQNyBUT",
        "outputId": "37db22f1-9fa8-49a3-83b0-10141c7657bb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6 if b > 6 else b):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(data[0][i][0], cmap=\"gray\" ,interpolation='none')\n",
        "    plt.title(f'Label: {classes[data[1][i]]}')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNTttyT9yBUV"
      },
      "outputs": [],
      "source": [
        "from TP5_MHA import MultiHeadAttention\n",
        "class VisionEncoder(torch.nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout):\n",
        "        super(VisionEncoder, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm1 = torch.nn.LayerNorm(self.embed_size)\n",
        "        self.norm2 = torch.nn.LayerNorm(self.embed_size)\n",
        "        self.attention = MultiHeadAttention(self.embed_size, self.num_heads, self.dropout)\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embed_size, embed_size*4),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(embed_size*4, embed_size),\n",
        "            torch.nn.Dropout(0.2)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.expand(32,-1,-1)\n",
        "        fwd_norm1 = self.norm1(x)\n",
        "        fwd_attention = fwd_norm1 + self.attention(fwd_norm1, fwd_norm1, fwd_norm1)\n",
        "        fwd_norm2 = self.norm2(fwd_attention)\n",
        "        fwd_mlp = fwd_attention + self.mlp(fwd_norm2)\n",
        "\n",
        "        return fwd_mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILy0TceFyBUV",
        "outputId": "57c1c2d1-db31-43ce-f712-64e39f82cc72"
      },
      "outputs": [],
      "source": [
        "class ViT(torch.nn.Module):\n",
        "    class_token = []\n",
        "    positional_encoding = []\n",
        "    def __init__(self, image_size, channel_size, patch_size, embed_size, nb_heads, classes, nb_layers, hidden_size, dropout):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_size = embed_size\n",
        "        self.nb_patches = (image_size // patch_size) ** 2\n",
        "        self.pixels_per_patch = channel_size * (patch_size ** 2)\n",
        "        self.nb_heads = nb_heads\n",
        "        self.classes = classes\n",
        "        self.nb_layers = nb_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "        self.class_token = torch.rand(1, 1, self.embed_size).to(device)\n",
        "        self.positional_encoding = torch.randn(1,self.nb_patches + 1,self.embed_size).to(device)\n",
        "        torch.nn.Parameter(self.class_token)\n",
        "        torch.nn.Parameter(self.positional_encoding)\n",
        "\n",
        "        self.embedding = torch.nn.Linear(self.pixels_per_patch,embed_size)\n",
        "        self.dropout_layer = torch.nn.Dropout(dropout)\n",
        "        self.encoders = torch.nn.ModuleList([])\n",
        "        for i in range(self.nb_layers):\n",
        "            self.encoders.append(VisionEncoder(self.embed_size, self.nb_heads, self.dropout))\n",
        "\n",
        "        self.norm = torch.nn.LayerNorm(self.embed_size)\n",
        "        self.classifier = torch.nn.Linear(self.embed_size, self.classes)\n",
        "\n",
        "\n",
        "    def forward(self, img_torch):\n",
        "        b, c, h, w = img_torch.size()\n",
        "        img_torch_reshape = img_torch.reshape(b, int((h / self.patch_size) * (w / self.patch_size)), c * self.patch_size * self.patch_size)\n",
        "        fwd_embeddings = self.embedding(img_torch_reshape)\n",
        "        self.class_token = self.class_token.expand(b, 1, self.embed_size).to(device)\n",
        "        fwd_cat_class_token = torch.cat((fwd_embeddings, self.class_token),1)\n",
        "\n",
        "        fwd_pos_encoding = fwd_cat_class_token + self.positional_encoding\n",
        "        fwd_dropout = self.dropout_layer(fwd_pos_encoding)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            fwd_dropout = encoder(fwd_dropout)\n",
        "\n",
        "        fwd_cls = fwd_dropout[:, -1]\n",
        "        fwd_cls = self.norm(fwd_cls)\n",
        "        fwd_cls = self.classifier(fwd_cls)\n",
        "        fwd_softmax = torch.nn.functional.log_softmax(fwd_cls, -1)\n",
        "        return fwd_softmax\n",
        "\n",
        "model = ViT(image_size=img_size, channel_size=channel_size, patch_size=25, embed_size=512, nb_heads=8, classes=len(classes), nb_layers=6, hidden_size=256, dropout=0.2).to(device)\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFaIP6LFKos3",
        "outputId": "750970c9-9bf1-4a69-fea7-3bbfd70bc320"
      },
      "outputs": [],
      "source": [
        "\n",
        "loss_fct = torch.nn.NLLLoss()\n",
        "print(loss_fct)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 5e-5)\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "u_YVT3HSyBUY",
        "outputId": "48b4537b-9d5a-4385-92b4-c57b535d803f"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "nb_epochs = 10\n",
        "def train_model():\n",
        "    for epoch in range(nb_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "\n",
        "        for batch_idx, (imgs, labels) in enumerate(train_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(imgs)\n",
        "            loss = loss_fct(predictions, labels)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            y_pred.extend(predictions.argmax(dim=1).tolist())  # Compléter ici (indice : on veut l'indice de la valeur maximale des éléments du tenseur pour chaque batch, une fonction PyTorch existe pour cela !)\n",
        "            y_true.extend(labels.tolist())\n",
        "\n",
        "            # Ajout de la valeur de loss du batch à la valeur de loss sur l'ensemble de l'epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Ajout de la loss de l'epoch à la liste de l'ensemble des loss\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "        # Vérification et calcul de la précision du modèle en comparant pour chaque image son label avec la valeur prédite\n",
        "        nb_imgs = len(y_pred)\n",
        "        total_correct = 0\n",
        "        for i in range(nb_imgs):\n",
        "            if y_pred[i] == y_true[i]:\n",
        "                total_correct += 1\n",
        "        accuracy = total_correct * 100 / nb_imgs\n",
        "\n",
        "        # Ajout de la précision à la liste des précisions\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        # Affichage des résultats pour l'epoch en cours (loss et précision)\n",
        "        print(\"----------\")\n",
        "        print(\"\")\n",
        "        print(\"Epoch:\", epoch)\n",
        "        print(\"\")\n",
        "        print(\"Loss:\", epoch_loss)\n",
        "        print(\"\")\n",
        "        print(f\"Accuracy: {accuracy} % ({total_correct} / {nb_imgs})\")\n",
        "        print(\"\")\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu1f-bwNyBUa"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5TObX-ryBUb"
      },
      "outputs": [],
      "source": [
        "plt.plot(accuracies)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train accuracy (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tIaqMaQyBUc"
      },
      "outputs": [],
      "source": [
        "def eval_model():\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        y_test_pred = []\n",
        "        y_test_true = []\n",
        "\n",
        "        print(test_dataloader)\n",
        "        for batch_idx, (imgs, labels) in enumerate(test_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            if(torch.Size([b]) == labels.size()):\n",
        "                predictions = model(imgs)\n",
        "                y_test_pred.extend(predictions.argmax(dim=1).tolist())\n",
        "                y_test_true.extend(labels.tolist())\n",
        "\n",
        "        nb_imgs = len(y_test_pred)\n",
        "        total_correct = 0\n",
        "        for i in range(nb_imgs):\n",
        "            if y_test_pred[i] == y_test_true[i]:\n",
        "                total_correct += 1\n",
        "        accuracy = total_correct * 100 / nb_imgs\n",
        "\n",
        "        print(f\"Evaluation accuracy: {accuracy} % ({total_correct} / {nb_imgs})\")\n",
        "eval_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
